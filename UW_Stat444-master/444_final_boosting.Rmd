---
output:
  pdf_document: 
    number_sections: true
---
<style type="text/css">

body{ /* Normal  */
      font-size: 1px;
  }
td {  /* Table  */
  font-size: 8px;
}
h1.title {
  font-size: 38px;
  color: DarkRed;
}
h1 { /* Header 1 */
  font-size: 28px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 22px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>

\begin{titlepage}

\center
\textsc{\LARGE University of Waterloo}\\[5cm]
{ \huge \bfseries STAT 444}\\[4cm]
     \textsc{\Large STAT 444 Spring 2019}\\[3cm]
     
\emph{Group 91:}\\[0.5cm]
Gengyao \textsc{Yuan}(20613017)\\[0.5cm]
Haohan  \textsc{Li}(20610397)


\end{titlepage}

\tableofcontents

\newpage
```{r setup,include=FALSE}
#library())
```

sample
\section{Executive summary:}
sample
\newpage
sample
\newline
sample
\section{Itroduction:}

sample
\section{Data}
sample
\section{preprocessing}
sample

\subsection{missing data}
sample
\subsection{outliers}
sample

\section{Smoothing methods}
sample

\section{Random  Forests}
sample

\section{Boosting}
The main purpose of using the boosting method is by giving heavier weight to some data we convert weak learners 
to stronger ones. In this section, we used **xgboost** (a gradient boosting library) to build the model.

**Note:** Xgboost could dealing with missing value by itself, so we don't need to worry about that


\subsection{Parameter Tuning}
Note that our final model is shown as below.

xgb.train(data=dtrain, max.depth=8, eta=0.02, nthread = 4, nrounds=1300, verbose = 0)

There are three important hyperparameter we can see here: **max.depth**, **eta**(learning rate), **nround**
In this section, we assue all of the folds has the smae distribution. So we use fold 1 to tune the hyper-parameter
to save some running time



\subsubsection{Max Depth}

max_depth is a hyperparameter indicates the maximum depth of a tree. 

By fixing other hyperparameters, we try different number of max depth with fold 2-5 as training set and 
fold 1 data as test set we cann see the test error changes.

![Testing error on Fold 1 data with differnt maximum depth](Visualization/BoostingDepth.jpg)

\subsubsection{Learning Rate}

**Learning Rate** (eta) is a tricky parameter. In **xgboost**, new trees are created to correct the errors from the predictions of exisiting trees. 
A big learning rate could make model fit the quicker (imagine we give huge weight to data that have prediction error)
A small learning rate could result in slow training and wasting of time.

It is pretty common to have small values, usually smaller than **0.2**. Also, this is a highly hardwared-dependent paramter. So, based on my computer, I chose **0.02**.

\subsubsection{nround}

**nround** stands for max number of boosting iterations. Too many iterations will caused overfitting, while too few iterations will caused underfitting.

Similar as previous subsections, we used fold 1 data as training set. 
So, we fixed all other hyperparameters and print out the training error and testing error at each iteration.


Clearly, after around 1300 round, model tends to have some overfitting behavior 

![Testing Error of XGBOOST in different iterations](Visualization/BoostingnroundTesting.jpg)


![Training Error of XGBOOST in different iterations](Visualization/BoostingnroundTraining.jpg)
Clearly, we can see training keep decreasing all the way till the end. However, traing error drop sharply till around 1300 iterations and grows up again. This is a sign of overfitting, therefore, we better stop running it before it gets overfit.
So we choose 1300 as nround parameter.


\subsection{Variable Selection}
![Variable Importance generated by xgboost](Visualization/BoostingVariableImportance.jpg)

Xgboost usually could select variable by itself, but it is always good to see the variable importance and base on that and other models to select most effective variables

\section{Aditional methods}
sample

\section{Statistical  Conclusions}
sample

\section{Future work}
sample

\section{Contribution}
sample

\section{Appendix}
sample


